#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœ€ç»ˆSEOæ£€æŸ¥è„šæœ¬
æ£€æŸ¥ç½‘ç«™æ‰€æœ‰HTMLæ–‡ä»¶çš„SEOä¼˜åŒ–æƒ…å†µ
"""

import os
import re
from bs4 import BeautifulSoup
import json

def check_seo_issues(file_path):
    """æ£€æŸ¥å•ä¸ªHTMLæ–‡ä»¶çš„SEOé—®é¢˜"""
    issues = []
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            soup = BeautifulSoup(content, 'html.parser')
            
        # æ£€æŸ¥æ ‡é¢˜
        title = soup.find('title')
        if not title:
            issues.append("ç¼ºå°‘titleæ ‡ç­¾")
        elif len(title.get_text()) > 60:
            issues.append(f"æ ‡é¢˜è¿‡é•¿({len(title.get_text())}å­—ç¬¦)")
        elif len(title.get_text()) < 30:
            issues.append(f"æ ‡é¢˜è¿‡çŸ­({len(title.get_text())}å­—ç¬¦)")
            
        # æ£€æŸ¥æè¿°
        description = soup.find('meta', attrs={'name': 'description'})
        if not description:
            issues.append("ç¼ºå°‘meta description")
        elif len(description.get('content', '')) > 160:
            issues.append(f"æè¿°è¿‡é•¿({len(description.get('content', ''))}å­—ç¬¦)")
        elif len(description.get('content', '')) < 120:
            issues.append(f"æè¿°è¿‡çŸ­({len(description.get('content', ''))}å­—ç¬¦)")
            
        # æ£€æŸ¥H1æ ‡ç­¾
        h1_tags = soup.find_all('h1')
        if not h1_tags:
            issues.append("ç¼ºå°‘H1æ ‡ç­¾")
        elif len(h1_tags) > 1:
            issues.append(f"H1æ ‡ç­¾è¿‡å¤š({len(h1_tags)}ä¸ª)")
            
        # æ£€æŸ¥å›¾ç‰‡altå±æ€§
        images = soup.find_all('img')
        missing_alt = 0
        for img in images:
            if not img.get('alt'):
                missing_alt += 1
        if missing_alt > 0:
            issues.append(f"{missing_alt}ä¸ªå›¾ç‰‡ç¼ºå°‘altå±æ€§")
            
        # æ£€æŸ¥canonicalé“¾æ¥
        canonical = soup.find('link', attrs={'rel': 'canonical'})
        if not canonical:
            issues.append("ç¼ºå°‘canonicalé“¾æ¥")
            
        # æ£€æŸ¥OGæ ‡ç­¾
        og_title = soup.find('meta', attrs={'property': 'og:title'})
        og_description = soup.find('meta', attrs={'property': 'og:description'})
        og_image = soup.find('meta', attrs={'property': 'og:image'})
        og_url = soup.find('meta', attrs={'property': 'og:url'})
        
        if not og_title:
            issues.append("ç¼ºå°‘og:title")
        if not og_description:
            issues.append("ç¼ºå°‘og:description")
        if not og_image:
            issues.append("ç¼ºå°‘og:image")
        if not og_url:
            issues.append("ç¼ºå°‘og:url")
            
        # æ£€æŸ¥Twitterå¡ç‰‡
        twitter_card = soup.find('meta', attrs={'name': 'twitter:card'})
        twitter_title = soup.find('meta', attrs={'name': 'twitter:title'})
        twitter_description = soup.find('meta', attrs={'name': 'twitter:description'})
        twitter_image = soup.find('meta', attrs={'name': 'twitter:image'})
        
        if not twitter_card:
            issues.append("ç¼ºå°‘Twitterå¡ç‰‡")
        if not twitter_title:
            issues.append("ç¼ºå°‘Twitteræ ‡é¢˜")
        if not twitter_description:
            issues.append("ç¼ºå°‘Twitteræè¿°")
        if not twitter_image:
            issues.append("ç¼ºå°‘Twitterå›¾ç‰‡")
            
        # æ£€æŸ¥ç»“æ„åŒ–æ•°æ®
        json_ld = soup.find('script', attrs={'type': 'application/ld+json'})
        if not json_ld:
            issues.append("ç¼ºå°‘JSON-LDç»“æ„åŒ–æ•°æ®")
        else:
            try:
                json.loads(json_ld.get_text())
            except json.JSONDecodeError:
                issues.append("JSON-LDæ ¼å¼é”™è¯¯")
                
    except Exception as e:
        issues.append(f"æ–‡ä»¶è¯»å–é”™è¯¯: {str(e)}")
        
    return issues

def main():
    """ä¸»å‡½æ•°"""
    base_dir = os.getcwd()
    total_files = 0
    files_with_issues = 0
    all_issues = {}
    
    print("å¼€å§‹æœ€ç»ˆSEOæ£€æŸ¥...")
    print("=" * 50)
    
    # éå†æ‰€æœ‰HTMLæ–‡ä»¶
    for root, dirs, files in os.walk(base_dir):
        for file in files:
            if file.endswith('.html'):
                file_path = os.path.join(root, file)
                rel_path = os.path.relpath(file_path, base_dir)
                
                total_files += 1
                issues = check_seo_issues(file_path)
                
                if issues:
                    files_with_issues += 1
                    all_issues[rel_path] = issues
                    print(f"\nâŒ {rel_path}:")
                    for issue in issues:
                        print(f"   - {issue}")
                else:
                    print(f"âœ… {rel_path}: SEOä¼˜åŒ–è‰¯å¥½")
    
    print("\n" + "=" * 50)
    print("SEOæ£€æŸ¥å®Œæˆ!")
    print(f"æ€»æ–‡ä»¶æ•°: {total_files}")
    print(f"æœ‰é—®é¢˜çš„æ–‡ä»¶: {files_with_issues}")
    print(f"SEOä¼˜åŒ–è‰¯å¥½çš„æ–‡ä»¶: {total_files - files_with_issues}")
    
    if all_issues:
        print("\néœ€è¦å…³æ³¨çš„ä¸»è¦é—®é¢˜:")
        issue_counts = {}
        for issues in all_issues.values():
            for issue in issues:
                issue_type = issue.split('(')[0] if '(' in issue else issue
                issue_counts[issue_type] = issue_counts.get(issue_type, 0) + 1
        
        for issue, count in sorted(issue_counts.items(), key=lambda x: x[1], reverse=True):
            print(f"  - {issue}: {count}æ¬¡")
    else:
        print("\nğŸ‰ æ‰€æœ‰æ–‡ä»¶çš„SEOä¼˜åŒ–éƒ½å¾ˆå®Œå–„!")

if __name__ == "__main__":
    main()