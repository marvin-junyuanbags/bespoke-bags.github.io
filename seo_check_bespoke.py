#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SEOæ£€æŸ¥è„šæœ¬ - bespoke-bags.com
æ£€æŸ¥ç½‘ç«™çš„SEOä¼˜åŒ–æƒ…å†µ
"""

import os
import re
from bs4 import BeautifulSoup
import json
from collections import defaultdict

def check_seo_issues(file_path):
    """æ£€æŸ¥å•ä¸ªHTMLæ–‡ä»¶çš„SEOé—®é¢˜"""
    issues = []
    
    try:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()
        
        soup = BeautifulSoup(content, 'html.parser')
        
        # æ£€æŸ¥åŸºæœ¬SEOå…ƒç´ 
        title = soup.find('title')
        if not title or not title.get_text().strip():
            issues.append('ç¼ºå°‘titleæ ‡ç­¾')
        elif len(title.get_text().strip()) > 60:
            issues.append('æ ‡é¢˜è¿‡é•¿')
        elif len(title.get_text().strip()) < 30:
            issues.append('æ ‡é¢˜è¿‡çŸ­')
        
        # æ£€æŸ¥meta description
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if not meta_desc or not meta_desc.get('content', '').strip():
            issues.append('ç¼ºå°‘meta description')
        elif len(meta_desc.get('content', '').strip()) > 160:
            issues.append('æè¿°è¿‡é•¿')
        elif len(meta_desc.get('content', '').strip()) < 120:
            issues.append('æè¿°è¿‡çŸ­')
        
        # æ£€æŸ¥H1æ ‡ç­¾
        h1_tags = soup.find_all('h1')
        if not h1_tags:
            issues.append('ç¼ºå°‘H1æ ‡ç­¾')
        elif len(h1_tags) > 1:
            issues.append('H1æ ‡ç­¾è¿‡å¤š')
        
        # æ£€æŸ¥canonicalé“¾æ¥
        canonical = soup.find('link', attrs={'rel': 'canonical'})
        if not canonical:
            issues.append('ç¼ºå°‘canonicalé“¾æ¥')
        
        # æ£€æŸ¥viewportè®¾ç½®
        viewport = soup.find('meta', attrs={'name': 'viewport'})
        if not viewport:
            issues.append('ç¼ºå°‘viewportè®¾ç½®')
        
        # æ£€æŸ¥meta keywords
        keywords = soup.find('meta', attrs={'name': 'keywords'})
        if not keywords or not keywords.get('content', '').strip():
            issues.append('ç¼ºå°‘meta keywords')
        
        # æ£€æŸ¥Open Graphæ ‡ç­¾
        og_title = soup.find('meta', attrs={'property': 'og:title'})
        og_desc = soup.find('meta', attrs={'property': 'og:description'})
        og_url = soup.find('meta', attrs={'property': 'og:url'})
        og_image = soup.find('meta', attrs={'property': 'og:image'})
        
        if not og_title:
            issues.append('ç¼ºå°‘og:title')
        if not og_desc:
            issues.append('ç¼ºå°‘og:description')
        if not og_url:
            issues.append('ç¼ºå°‘og:url')
        if not og_image:
            issues.append('ç¼ºå°‘og:image')
        
        # æ£€æŸ¥Twitter Cardæ ‡ç­¾
        twitter_card = soup.find('meta', attrs={'name': 'twitter:card'})
        twitter_title = soup.find('meta', attrs={'name': 'twitter:title'})
        twitter_desc = soup.find('meta', attrs={'name': 'twitter:description'})
        twitter_image = soup.find('meta', attrs={'name': 'twitter:image'})
        
        if not twitter_card:
            issues.append('ç¼ºå°‘Twitterå¡ç‰‡')
        if not twitter_title:
            issues.append('ç¼ºå°‘Twitteræ ‡é¢˜')
        if not twitter_desc:
            issues.append('ç¼ºå°‘Twitteræè¿°')
        if not twitter_image:
            issues.append('ç¼ºå°‘Twitterå›¾ç‰‡')
        
        # æ£€æŸ¥JSON-LDç»“æ„åŒ–æ•°æ®
        json_ld = soup.find('script', attrs={'type': 'application/ld+json'})
        if not json_ld:
            issues.append('ç¼ºå°‘JSON-LDç»“æ„åŒ–æ•°æ®')
        
    except Exception as e:
        issues.append(f'æ–‡ä»¶è¯»å–é”™è¯¯: {str(e)}')
    
    return issues

def main():
    """ä¸»å‡½æ•°"""
    website_dir = '.'
    total_files = 0
    files_with_issues = 0
    issue_counts = defaultdict(int)
    
    print("å¼€å§‹æ£€æŸ¥ bespoke-bags.com ç½‘ç«™çš„SEOé—®é¢˜...")
    print("=" * 50)
    
    # éå†æ‰€æœ‰HTMLæ–‡ä»¶
    for root, dirs, files in os.walk(website_dir):
        for file in files:
            if file.endswith('.html'):
                file_path = os.path.join(root, file)
                total_files += 1
                
                issues = check_seo_issues(file_path)
                
                if issues:
                    files_with_issues += 1
                    for issue in issues:
                        issue_counts[issue] += 1
    
    # è¾“å‡ºç»Ÿè®¡ç»“æœ
    print(f"\næ£€æŸ¥å®Œæˆï¼")
    print(f"æ€»æ–‡ä»¶æ•°: {total_files}")
    print(f"æœ‰é—®é¢˜çš„æ–‡ä»¶æ•°: {files_with_issues}")
    print(f"SEOä¼˜åŒ–è‰¯å¥½çš„æ–‡ä»¶æ•°: {total_files - files_with_issues}")
    print(f"SEOä¼˜åŒ–ç‡: {((total_files - files_with_issues) / total_files * 100):.1f}%")
    
    if issue_counts:
        print("\nä¸»è¦SEOé—®é¢˜ç»Ÿè®¡:")
        print("-" * 30)
        for issue, count in sorted(issue_counts.items(), key=lambda x: x[1], reverse=True):
            print(f"{issue}: {count}æ¬¡")
    else:
        print("\nğŸ‰ æ­å–œï¼æ‰€æœ‰æ–‡ä»¶çš„SEOéƒ½å·²ä¼˜åŒ–å®Œæˆï¼")

if __name__ == '__main__':
    main()