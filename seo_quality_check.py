#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
SEOè´¨é‡æ£€æŸ¥è„šæœ¬
æ£€æŸ¥æ‰€æœ‰HTMLæ–‡ä»¶çš„SEOå…ƒç´ å®Œæ•´æ€§
"""

import os
import re
from bs4 import BeautifulSoup
import json

def check_seo_elements(file_path):
    """æ£€æŸ¥å•ä¸ªHTMLæ–‡ä»¶çš„SEOå…ƒç´ """
    issues = []
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            soup = BeautifulSoup(content, 'html.parser')
        
        # æ£€æŸ¥åŸºæœ¬SEOå…ƒç´ 
        title = soup.find('title')
        if not title or len(title.get_text().strip()) < 10:
            issues.append("æ ‡é¢˜ç¼ºå¤±æˆ–è¿‡çŸ­")
        elif len(title.get_text().strip()) > 60:
            issues.append("æ ‡é¢˜è¿‡é•¿ï¼ˆ>60å­—ç¬¦ï¼‰")
        
        # æ£€æŸ¥metaæè¿°
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if not meta_desc or len(meta_desc.get('content', '').strip()) < 120:
            issues.append("Metaæè¿°ç¼ºå¤±æˆ–è¿‡çŸ­")
        elif len(meta_desc.get('content', '').strip()) > 160:
            issues.append("Metaæè¿°è¿‡é•¿ï¼ˆ>160å­—ç¬¦ï¼‰")
        
        # æ£€æŸ¥å…³é”®è¯
        meta_keywords = soup.find('meta', attrs={'name': 'keywords'})
        if not meta_keywords:
            issues.append("Metaå…³é”®è¯ç¼ºå¤±")
        
        # æ£€æŸ¥canonicalé“¾æ¥
        canonical = soup.find('link', attrs={'rel': 'canonical'})
        if not canonical:
            issues.append("Canonicalé“¾æ¥ç¼ºå¤±")
        
        # æ£€æŸ¥Open Graphæ ‡ç­¾
        og_title = soup.find('meta', attrs={'property': 'og:title'})
        og_desc = soup.find('meta', attrs={'property': 'og:description'})
        og_image = soup.find('meta', attrs={'property': 'og:image'})
        
        if not og_title:
            issues.append("OGæ ‡é¢˜ç¼ºå¤±")
        if not og_desc:
            issues.append("OGæè¿°ç¼ºå¤±")
        if not og_image:
            issues.append("OGå›¾ç‰‡ç¼ºå¤±")
        
        # æ£€æŸ¥Twitterå¡ç‰‡
        twitter_card = soup.find('meta', attrs={'name': 'twitter:card'})
        if not twitter_card:
            issues.append("Twitterå¡ç‰‡ç¼ºå¤±")
        
        # æ£€æŸ¥ç»“æ„åŒ–æ•°æ®
        structured_data = soup.find('script', attrs={'type': 'application/ld+json'})
        if not structured_data:
            issues.append("ç»“æ„åŒ–æ•°æ®ç¼ºå¤±")
        else:
            try:
                json.loads(structured_data.get_text())
            except json.JSONDecodeError:
                issues.append("ç»“æ„åŒ–æ•°æ®æ ¼å¼é”™è¯¯")
        
        # æ£€æŸ¥H1æ ‡ç­¾
        h1_tags = soup.find_all('h1')
        if len(h1_tags) == 0:
            issues.append("H1æ ‡ç­¾ç¼ºå¤±")
        elif len(h1_tags) > 1:
            issues.append("å¤šä¸ªH1æ ‡ç­¾")
        
        # æ£€æŸ¥å›¾ç‰‡altå±æ€§
        images = soup.find_all('img')
        images_without_alt = [img for img in images if not img.get('alt')]
        if images_without_alt:
            issues.append(f"{len(images_without_alt)}å¼ å›¾ç‰‡ç¼ºå°‘altå±æ€§")
        
        # æ£€æŸ¥å†…éƒ¨é“¾æ¥
        internal_links = soup.find_all('a', href=True)
        broken_links = []
        for link in internal_links:
            href = link.get('href')
            if href.startswith('../') or href.startswith('./'):
                # æ£€æŸ¥ç›¸å¯¹é“¾æ¥æ˜¯å¦å­˜åœ¨
                link_path = os.path.normpath(os.path.join(os.path.dirname(file_path), href))
                if not os.path.exists(link_path):
                    broken_links.append(href)
        
        if broken_links:
            issues.append(f"å‘ç°{len(broken_links)}ä¸ªæŸåçš„å†…éƒ¨é“¾æ¥")
        
    except Exception as e:
        issues.append(f"æ–‡ä»¶è¯»å–é”™è¯¯: {str(e)}")
    
    return issues

def main():
    """ä¸»å‡½æ•°"""
    blog_dir = 'blog'
    results = {}
    total_files = 0
    files_with_issues = 0
    
    print("å¼€å§‹SEOè´¨é‡æ£€æŸ¥...\n")
    
    # æ£€æŸ¥åšå®¢ç›®å½•ä¸‹çš„æ‰€æœ‰HTMLæ–‡ä»¶
    for filename in os.listdir(blog_dir):
        if filename.endswith('.html') and filename != 'index.html':
            file_path = os.path.join(blog_dir, filename)
            total_files += 1
            
            issues = check_seo_elements(file_path)
            
            if issues:
                files_with_issues += 1
                results[filename] = issues
                print(f"âŒ {filename}:")
                for issue in issues:
                    print(f"   - {issue}")
                print()
            else:
                print(f"âœ… {filename}: æ‰€æœ‰SEOå…ƒç´ æ­£å¸¸")
    
    # è¾“å‡ºæ€»ç»“
    print(f"\n=== SEOæ£€æŸ¥æ€»ç»“ ===")
    print(f"æ€»æ–‡ä»¶æ•°: {total_files}")
    print(f"æœ‰é—®é¢˜çš„æ–‡ä»¶: {files_with_issues}")
    print(f"é€šè¿‡ç‡: {((total_files - files_with_issues) / total_files * 100):.1f}%")
    
    if results:
        print(f"\néœ€è¦ä¿®å¤çš„é—®é¢˜:")
        all_issues = []
        for issues in results.values():
            all_issues.extend(issues)
        
        issue_counts = {}
        for issue in all_issues:
            issue_counts[issue] = issue_counts.get(issue, 0) + 1
        
        for issue, count in sorted(issue_counts.items(), key=lambda x: x[1], reverse=True):
            print(f"  - {issue}: {count}æ¬¡")
    
    return len(results) == 0

if __name__ == '__main__':
    success = main()
    if success:
        print("\nğŸ‰ æ‰€æœ‰æ–‡ä»¶SEOæ£€æŸ¥é€šè¿‡ï¼")
    else:
        print("\nâš ï¸  å‘ç°SEOé—®é¢˜ï¼Œè¯·æŸ¥çœ‹ä¸Šè¿°æŠ¥å‘Šè¿›è¡Œä¿®å¤ã€‚")