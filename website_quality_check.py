#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç½‘ç«™è´¨é‡æ£€æŸ¥è„šæœ¬
æ£€æŸ¥ç½‘ç«™çš„æ•´ä½“è´¨é‡ã€æ€§èƒ½å’Œå¯ç”¨æ€§
"""

import os
import re
from bs4 import BeautifulSoup
import json
from urllib.parse import urljoin, urlparse

def check_page_performance(file_path):
    """æ£€æŸ¥é¡µé¢æ€§èƒ½ç›¸å…³æŒ‡æ ‡"""
    issues = []
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            soup = BeautifulSoup(content, 'html.parser')
        
        # æ£€æŸ¥CSSæ–‡ä»¶æ•°é‡
        css_links = soup.find_all('link', rel='stylesheet')
        if len(css_links) > 3:
            issues.append(f"CSSæ–‡ä»¶è¿‡å¤š({len(css_links)}ä¸ª)ï¼Œå¯èƒ½å½±å“åŠ è½½é€Ÿåº¦")
        
        # æ£€æŸ¥JavaScriptæ–‡ä»¶
        js_scripts = soup.find_all('script', src=True)
        if len(js_scripts) > 5:
            issues.append(f"JavaScriptæ–‡ä»¶è¿‡å¤š({len(js_scripts)}ä¸ª)")
        
        # æ£€æŸ¥å›¾ç‰‡ä¼˜åŒ–
        images = soup.find_all('img')
        large_images = []
        for img in images:
            src = img.get('src', '')
            if src and not src.endswith('.webp') and not src.startswith('data:'):
                if any(ext in src.lower() for ext in ['.jpg', '.jpeg', '.png']):
                    large_images.append(src)
        
        if large_images:
            issues.append(f"å‘ç°{len(large_images)}å¼ æœªä¼˜åŒ–å›¾ç‰‡(éWebPæ ¼å¼)")
        
        # æ£€æŸ¥å†…è”æ ·å¼
        inline_styles = soup.find_all(attrs={'style': True})
        if len(inline_styles) > 10:
            issues.append(f"å†…è”æ ·å¼è¿‡å¤š({len(inline_styles)}ä¸ª)ï¼Œå»ºè®®ç§»è‡³CSSæ–‡ä»¶")
        
    except Exception as e:
        issues.append(f"æ€§èƒ½æ£€æŸ¥é”™è¯¯: {str(e)}")
    
    return issues

def check_accessibility(file_path):
    """æ£€æŸ¥å¯è®¿é—®æ€§"""
    issues = []
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            soup = BeautifulSoup(content, 'html.parser')
        
        # æ£€æŸ¥å›¾ç‰‡altå±æ€§
        images = soup.find_all('img')
        missing_alt = [img for img in images if not img.get('alt')]
        if missing_alt:
            issues.append(f"{len(missing_alt)}å¼ å›¾ç‰‡ç¼ºå°‘altå±æ€§")
        
        # æ£€æŸ¥è¡¨å•æ ‡ç­¾
        inputs = soup.find_all(['input', 'textarea', 'select'])
        unlabeled_inputs = []
        for inp in inputs:
            input_id = inp.get('id')
            if input_id:
                label = soup.find('label', attrs={'for': input_id})
                if not label:
                    unlabeled_inputs.append(input_id)
            elif not inp.get('aria-label') and not inp.get('placeholder'):
                unlabeled_inputs.append('unnamed')
        
        if unlabeled_inputs:
            issues.append(f"{len(unlabeled_inputs)}ä¸ªè¡¨å•å…ƒç´ ç¼ºå°‘æ ‡ç­¾")
        
        # æ£€æŸ¥æ ‡é¢˜å±‚çº§
        headings = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
        if headings:
            heading_levels = [int(h.name[1]) for h in headings]
            for i in range(1, len(heading_levels)):
                if heading_levels[i] - heading_levels[i-1] > 1:
                    issues.append("æ ‡é¢˜å±‚çº§è·³è·ƒï¼Œå½±å“å¯è®¿é—®æ€§")
                    break
        
        # æ£€æŸ¥é“¾æ¥æ–‡æœ¬
        links = soup.find_all('a')
        generic_links = []
        for link in links:
            text = link.get_text().strip().lower()
            if text in ['click here', 'read more', 'more', 'here', 'ç‚¹å‡»è¿™é‡Œ', 'æ›´å¤š', 'é˜…è¯»æ›´å¤š']:
                generic_links.append(text)
        
        if generic_links:
            issues.append(f"{len(generic_links)}ä¸ªé“¾æ¥ä½¿ç”¨äº†é€šç”¨æ–‡æœ¬")
        
    except Exception as e:
        issues.append(f"å¯è®¿é—®æ€§æ£€æŸ¥é”™è¯¯: {str(e)}")
    
    return issues

def check_content_quality(file_path):
    """æ£€æŸ¥å†…å®¹è´¨é‡"""
    issues = []
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            soup = BeautifulSoup(content, 'html.parser')
        
        # æå–ä¸»è¦å†…å®¹
        main_content = soup.find('main') or soup.find('article') or soup.find('div', class_=re.compile(r'content|post|article'))
        
        if main_content:
            text_content = main_content.get_text().strip()
            word_count = len(text_content.split())
            
            if word_count < 300:
                issues.append(f"å†…å®¹è¿‡çŸ­({word_count}è¯)ï¼Œå»ºè®®è‡³å°‘300è¯")
            elif word_count > 3000:
                issues.append(f"å†…å®¹è¿‡é•¿({word_count}è¯)ï¼Œè€ƒè™‘åˆ†é¡µ")
            
            # æ£€æŸ¥æ®µè½é•¿åº¦
            paragraphs = main_content.find_all('p')
            long_paragraphs = [p for p in paragraphs if len(p.get_text().split()) > 100]
            if long_paragraphs:
                issues.append(f"{len(long_paragraphs)}ä¸ªæ®µè½è¿‡é•¿ï¼Œå½±å“é˜…è¯»ä½“éªŒ")
            
            # æ£€æŸ¥åˆ—è¡¨ä½¿ç”¨
            lists = main_content.find_all(['ul', 'ol'])
            if word_count > 500 and len(lists) == 0:
                issues.append("é•¿æ–‡ç« å»ºè®®ä½¿ç”¨åˆ—è¡¨æé«˜å¯è¯»æ€§")
        
        # æ£€æŸ¥é‡å¤å†…å®¹
        title = soup.find('title')
        h1 = soup.find('h1')
        if title and h1:
            title_text = title.get_text().strip()
            h1_text = h1.get_text().strip()
            if title_text.lower() == h1_text.lower():
                issues.append("æ ‡é¢˜å’ŒH1å®Œå…¨ç›¸åŒï¼Œå»ºè®®æœ‰æ‰€åŒºåˆ«")
        
    except Exception as e:
        issues.append(f"å†…å®¹è´¨é‡æ£€æŸ¥é”™è¯¯: {str(e)}")
    
    return issues

def check_mobile_friendliness(file_path):
    """æ£€æŸ¥ç§»åŠ¨ç«¯å‹å¥½æ€§"""
    issues = []
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            soup = BeautifulSoup(content, 'html.parser')
        
        # æ£€æŸ¥viewport metaæ ‡ç­¾
        viewport = soup.find('meta', attrs={'name': 'viewport'})
        if not viewport:
            issues.append("ç¼ºå°‘viewport metaæ ‡ç­¾")
        else:
            viewport_content = viewport.get('content', '')
            if 'width=device-width' not in viewport_content:
                issues.append("viewportè®¾ç½®ä¸æ­£ç¡®")
        
        # æ£€æŸ¥å›ºå®šå®½åº¦å…ƒç´ 
        style_tags = soup.find_all('style')
        inline_styles = soup.find_all(attrs={'style': True})
        
        fixed_width_pattern = re.compile(r'width\s*:\s*\d+px')
        for tag in style_tags + inline_styles:
            style_content = tag.get('style', '') if hasattr(tag, 'get') else str(tag)
            if fixed_width_pattern.search(style_content):
                issues.append("å‘ç°å›ºå®šåƒç´ å®½åº¦ï¼Œå¯èƒ½å½±å“ç§»åŠ¨ç«¯æ˜¾ç¤º")
                break
        
        # æ£€æŸ¥å­—ä½“å¤§å°
        small_font_pattern = re.compile(r'font-size\s*:\s*(\d+)px')
        for tag in style_tags + inline_styles:
            style_content = tag.get('style', '') if hasattr(tag, 'get') else str(tag)
            matches = small_font_pattern.findall(style_content)
            for match in matches:
                if int(match) < 14:
                    issues.append("å­—ä½“è¿‡å°ï¼Œç§»åŠ¨ç«¯å¯è¯»æ€§å·®")
                    break
        
    except Exception as e:
        issues.append(f"ç§»åŠ¨ç«¯æ£€æŸ¥é”™è¯¯: {str(e)}")
    
    return issues

def check_single_file(file_path):
    """æ£€æŸ¥å•ä¸ªæ–‡ä»¶çš„æ‰€æœ‰è´¨é‡æŒ‡æ ‡"""
    all_issues = {
        'performance': check_page_performance(file_path),
        'accessibility': check_accessibility(file_path),
        'content': check_content_quality(file_path),
        'mobile': check_mobile_friendliness(file_path)
    }
    
    return all_issues

def main():
    """ä¸»å‡½æ•°"""
    directories_to_check = ['blog']
    total_files = 0
    files_with_issues = 0
    all_issues = {
        'performance': [],
        'accessibility': [],
        'content': [],
        'mobile': []
    }
    
    print("å¼€å§‹ç½‘ç«™è´¨é‡æ£€æŸ¥...\n")
    
    for directory in directories_to_check:
        if not os.path.exists(directory):
            continue
            
        for filename in os.listdir(directory):
            if filename.endswith('.html'):
                file_path = os.path.join(directory, filename)
                total_files += 1
                
                file_issues = check_single_file(file_path)
                
                has_issues = any(issues for issues in file_issues.values())
                
                if has_issues:
                    files_with_issues += 1
                    print(f"âš ï¸  {filename}:")
                    
                    for category, issues in file_issues.items():
                        if issues:
                            print(f"  {category.upper()}:")
                            for issue in issues:
                                print(f"    - {issue}")
                                all_issues[category].append(issue)
                    print()
                else:
                    print(f"âœ… {filename}: è´¨é‡æ£€æŸ¥é€šè¿‡")
    
    # è¾“å‡ºæ€»ç»“
    print(f"\n=== ç½‘ç«™è´¨é‡æ£€æŸ¥æ€»ç»“ ===")
    print(f"æ€»æ–‡ä»¶æ•°: {total_files}")
    print(f"æœ‰é—®é¢˜çš„æ–‡ä»¶: {files_with_issues}")
    print(f"é€šè¿‡ç‡: {((total_files - files_with_issues) / total_files * 100):.1f}%")
    
    print(f"\n=== é—®é¢˜åˆ†ç±»ç»Ÿè®¡ ===")
    for category, issues in all_issues.items():
        if issues:
            print(f"\n{category.upper()} ({len(issues)}ä¸ªé—®é¢˜):")
            issue_counts = {}
            for issue in issues:
                issue_counts[issue] = issue_counts.get(issue, 0) + 1
            
            for issue, count in sorted(issue_counts.items(), key=lambda x: x[1], reverse=True)[:5]:
                print(f"  - {issue}: {count}æ¬¡")
    
    return files_with_issues == 0

if __name__ == '__main__':
    success = main()
    if success:
        print("\nğŸ‰ ç½‘ç«™è´¨é‡æ£€æŸ¥å…¨éƒ¨é€šè¿‡ï¼")
    else:
        print("\nğŸ“‹ ç½‘ç«™è´¨é‡æ£€æŸ¥å®Œæˆï¼Œè¯·æŸ¥çœ‹ä¸Šè¿°æŠ¥å‘Šã€‚")